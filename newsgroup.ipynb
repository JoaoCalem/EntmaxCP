{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/margaridacampos/.pyenv/versions/entmaxcp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Label index: 10\n",
      "Label name: rec.sport.hockey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/margaridacampos/.pyenv/versions/entmaxcp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 15076/15076 [00:20<00:00, 727.98it/s]\n",
      "100%|██████████| 3770/3770 [00:05<00:00, 738.70it/s] \n"
     ]
    }
   ],
   "source": [
    "# Load the 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "data = pd.DataFrame({'text_data': newsgroups.data, 'label': newsgroups.target})\n",
    "\n",
    "# Visualize newsgroup data object\n",
    "entry_index = 0\n",
    "print(f\"Text:\\n{newsgroups['data'][entry_index]}\\n\\n\")\n",
    "print(f\"Label index: {newsgroups['target'][entry_index]}\")\n",
    "print(f\"Label name: {newsgroups['target_names'][newsgroups['target'][entry_index]]}\")\n",
    "\n",
    "# Shuffle the dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split the dataset into training and validation sets (80:20 ratio)\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize BERT tokenizer using the pretrained 'bert-base-uncased' model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "max_seq_len = 128\n",
    "\n",
    "def tokenize_data(data, tokenizer, max_seq_len):\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "\n",
    "    # Iterate through each row in the dataset\n",
    "    for index, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        # Tokenize the text using BERT's tokenizer with additional parameters\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            row[\"text_data\"],\n",
    "            add_special_tokens=True,  # Add [CLS] and [SEP] tokens\n",
    "            max_length=max_seq_len,  # Set max sequence length to 128\n",
    "            padding=\"max_length\",  # Pad shorter sequences to max_seq_len\n",
    "            truncation=True,  # Truncate longer sequences to max_seq_len\n",
    "            return_attention_mask=True,  # Return attention masks\n",
    "        )\n",
    "\n",
    "        # Append tokenized data to respective lists\n",
    "        input_ids.append(encoded[\"input_ids\"])\n",
    "        attention_masks.append(encoded[\"attention_mask\"])\n",
    "        labels.append(row[\"label\"])\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(labels)\n",
    "\n",
    "# Tokenize both the training and validation data using the defined function\n",
    "train_input_ids, train_attention_masks, train_labels = tokenize_data(train_data, tokenizer, max_seq_len)\n",
    "val_input_ids, val_attention_masks, val_labels = tokenize_data(val_data, tokenizer, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Create a TensorDataset object for the training set\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "# Use RandomSampler to shuffle the samples in the dataset\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "# Create DataLoader for the training set using dataset, sampler, and batch size\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create a TensorDataset object for the validation set\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "# Use SequentialSampler to process the validation dataset sequentially\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "# Create DataLoader for the validation set using dataset, sampler, and batch size\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load the pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=20,  # Number of labels (20) corresponds to the 20 newsgroups dataset\n",
    "    output_attentions=False,  # Do not output attention weights\n",
    "    output_hidden_states=False,  # Do not output hidden states\n",
    ")\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/margaridacampos/.pyenv/versions/entmaxcp/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training - Loss: 3.1884:   1%|          | 5/943 [00:13<39:04,  2.50s/it]  "
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "num_epochs = 3\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "# Create the optimizer and scheduler for fine-tuning the model\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Use a progress bar during training\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", position=0, leave=True)\n",
    "\n",
    "    # Iterate through each batch in a training epoch\n",
    "    for batch in progress_bar:\n",
    "        input_ids, attention_masks, labels = [t.to(device) for t in batch]\n",
    "\n",
    "        # Zero out gradients before each backward pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to compute the outputs and loss\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass and update optimizer/scheduler steps\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        progress_bar.set_description(f\"Training - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "\n",
    "    # Use a progress bar during evaluation\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluation\", position=0, leave=True)\n",
    "\n",
    "    # Iterate through each batch in a validation epoch\n",
    "    for batch in progress_bar:\n",
    "        input_ids, attention_masks, labels = [t.to(device) for t in batch]\n",
    "\n",
    "        # Disable gradient calculations during evaluation\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "        logits = outputs[0].detach().cpu().numpy()\n",
    "        label_ids = labels.cpu().numpy()\n",
    "\n",
    "        # Calculate accuracy for the current batch\n",
    "        batch_accuracy = accuracy_score(label_ids, logits.argmax(axis=-1))\n",
    "        total_eval_accuracy += batch_accuracy\n",
    "\n",
    "        progress_bar.set_description(f\"Evaluation - Batch Accuracy: {batch_accuracy:.4f}\")\n",
    "\n",
    "    return total_eval_accuracy / len(dataloader)\n",
    "\n",
    "# Train and evaluate the model for 'num_epochs' times\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
    "    val_accuracy = evaluate(model, val_dataloader, device)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Loss: {train_loss:.4f} - Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entmaxcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
